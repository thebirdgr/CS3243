What is rational at any given time depends on four things:
‚Ä¢ The performance measure that defines the criterion of success.
‚Ä¢ The agent‚Äôs prior knowledge of the environment.
‚Ä¢ The actions that the agent can perform.
‚Ä¢ The agent‚Äôs percept sequence to date.

An omniscientOMNISCIENCE
agent knows the actual outcome of its actions and can act accordingly; but omniscience is
impossible in reality. 
Rationality max-imizes expected performance, while perfection maximizes actual performance.
Actions shoud be chosen to maximize expected performance, which is rational. Some actions can increase percepts, which allows for better decision making.
 So, just as evolution provides animals with enough built-in reflexes to survive long enough to learn for themselves, it would be reasonable to provide an artificial intelligent agent with some initial knowledge as well as an ability to learn.	
Deterministic vs. stochastic. If the next state of the environment is completely deter-DETERMINISTIC
STOCHASTIC mined by the current state and the action executed by the agent, then we say the environment
is deterministic; otherwise, it is stochastic
 a nondeterministic environment is one in which actions areNONDETERMINISTIC
characterized by their possible outcomes, but no probabilities are attached to them.
If the environment itself does not change with the passage of time but the agent‚Äôs performance score does, then we say the environment is semidynamic. 
Discrete vs. continuous: The discrete/continuous distinction applies to the state of theDISCRETE
CONTINUOUS environment, to the way time is handled, and to the percepts and actions of the agent

 simple reflex agent. These agents select actions on the basis SIMPLE REFLEX AGENT of the current percept, ignoring the rest of the percept history,only if the environment is fully observable. Escape from infinite loops is possible if the agent can randomize its actions. In single-agent environments, randomization is usually not rational. 
model based agents are one's that react to whatever percepts it sees at the moment and not seeking a goal. It's ideal next state would be one that satisfies the performance metrics of the agent program. 
An agent‚Äôs utility function is essentially an internalizationUTILITY FUNCTION
of the performance measure. 
The critic tells the learning element how well the agent is doing with respect to a fixed
performance standard. The critic is necessary because the percepts themselves provide no
indication of the agent‚Äôs success. 

we say that an agent‚Äôs behavior is described by the agent function that maps any given percept sequence to an action.

Internally, the agent function for an artificial agent will be implemented by an agent program. It is important to keep these two ideas distinct. The agent function is an abstract mathematical description; the agent program is a concrete implementation, running within some physical system.



Searching:
The set of all leaf nodes available for expansion at any given point is called the frontier.

try Exercise 3.14 now.
Completeness: Is the algorithm guaranteed to Ô¨Ånd a solution when there is one?
‚Ä¢ Optimality: Does the strategy Ô¨Ånd the optimal solution, as deÔ¨Åned on page 68?
‚Ä¢ Time complexity: How long does it take to Ô¨Ånd a solution?
‚Ä¢ Space complexity: How much memory is needed to perform the search?

Completeness: Is the algorithm guaranteed to Ô¨Ånd a solution when there is one?
‚Ä¢ Optimality: Does the strategy Ô¨Ånd the optimal solution, as deÔ¨Åned on page 68?
‚Ä¢ Time complexity: How long does it take to Ô¨Ånd a solution?
‚Ä¢ Space complexity: How much memory is needed to perform the search?


BFS:
Completeness: Is the algorithm guaranteed to Ô¨Ånd a solution when there is one?
‚Ä¢ Optimality: Does the strategy Ô¨Ånd the optimal solution, as deÔ¨Åned on page 68?
‚Ä¢ Time complexity: How long does it take to Ô¨Ånd a solution?
‚Ä¢ Space complexity: How much memory is needed to perform the search?
BFS is complete, shallowest goal node is not necessarily the optimal one If the algorithm were to apply the goal test to nodes when selected for expansion, rather than
when generated, the whole layer of nodes at depth d would be expanded before the goal was
detected and the time complexity would be O(bd+1 ).
There will be O(bd‚àí1 ) nodes in the explored set and O(bd ) nodes in the frontier, so the space complexity is O(bd ), i.e., it is dominated by the size of the frontier.

exponential-complexity search problems cannot be solved by uninformed methods for any but the smallest instances.

uniform-cost search is optimal in general., priority-queue Then,
because step costs are nonnegative, paths never get shorter as nodes are added. These two
facts together imply that uniform-cost search expands nodes in order of their optimal path
cost. Hence, the Ô¨Årst goal node selected for expansion must be the optimal solution. Completeness is
guaranteed provided the cost of every step exceeds some small positive constant «´, unless there is a negative number then an infinte loop could happen. 

Instead, let C ‚àó be the cost of the optimal solution,7
and assume that every action costs at least «´. Then the algorithm‚Äôs worst-case time and space complexity is O(b1+‚åäC /«´‚åã ), which can be much greater than bd . This is because uniform-
cost search can explore large trees of small steps before exploring paths involving large and perhaps useful steps.

When all step
costs are the same, uniform-cost search is similar to breadth-Ô¨Årst search, except that the latter
stops as soon as it generates a goal, whereas uniform-cost search examines all the nodes at
the goal‚Äôs depth to see if one has a lower cost; thus uniform-cost search does strictly more
work by expanding nodes at depth d unnecessarily.
Exercise 3.18.

depth-Ô¨Årst search is not optimal., complete for finte graphs, incomplete for trees

best-Ô¨Årst search. Best-Ô¨Årst search is an
instance of the general T REE -S EARCH or G RAPH -S EARCH algorithm in which a node is
selected for expansion based on an evaluation function, f (n).

Greedy best-Ô¨Årst tree search is also incomplete even in a Ô¨Ånite state space, much like depth-Ô¨Årst search.
The
worst-case time and space complexity for the tree version is O(bm ), where m is the maximum
depth of the search space. With a good heuristic function, however, the complexity can be
reduced substantially. The amount of the reduction depends on the particular problem and on
the quality of the heuristic.

A‚àó search isboth complete and optimal. The algorithm is identical to U NIFORM -C OST-S EARCH except that A‚àó uses g + h instead of g.

A heuristic h(n) is consistent if, for
every node n and every successor n‚Ä≤ of n generated by any action a, the estimated cost of
reaching the goal from n is no greater than the step cost of getting to n‚Ä≤ plus the estimated
cost of reaching the goal from n‚Ä≤ :
h(n) ‚â§ c(n, a, n‚Ä≤ ) + h(n‚Ä≤ )
(Exercise 3.32)
It is fairly easy to show (Exercise 3.32) that every consistent heuristic is also admissible.
Consistency is therefore a stricter requirement than admissibility, but one has to work quite
hard to concoct heuristics that are admissible but not consistent.

the tree-search version of A‚àó is
optimal if h(n) is admissible, while the graph-search version is optimal if h(n) is consistent.

The Ô¨Årst step is to establish the following: if h(n) is consistent, then the values of
f (n) along any path are nondecreasing.The next step is to prove that whenever A‚àó selects a node n for expansion, the optimal path
to that node has been found. Were this not the case, there would have to be another frontier
node n‚Ä≤ on the optimal path from the start node to n, by the graph separation property, because f is nondecreasing along any path, n‚Ä≤ would have lower f -cost than n
and would have been selected Ô¨Årst.

If C ‚àó is the cost of the
optimal solution path, then we can say the following:
‚Ä¢ A‚àó expands all nodes with f (n) < C ‚àó .
‚Ä¢ A‚àó might then expand some of the nodes right on the ‚Äúgoal contour‚Äù (where f (n) = C ‚àó )
\before selecting a goal node.
Completeness requires that there be only Ô¨Ånitely many nodes with cost less than or equal to
C ‚àó , a condition that is true if all step costs exceed some Ô¨Ånite «´ and if b is Ô¨Ånite.
Opyimally efficient This is because any algorithm that does not expand all nodes with f (n) < C ‚àó
runs the risk of missing the optimal solution.

The absolute error is
deÔ¨Åned as ‚àÜ ‚â° h‚àó ‚àí h, where h‚àó is the actual cost of getting from the root to the goal, and
the relative error is deÔ¨Åned as «´ ‚â° (h‚àó ‚àí h)/h‚àó . , the time complexity of A‚àó is exponential in the maximum absolute error, that is,
O(b‚àÜ ). For constant step costs, we can write this as O(b«´d ), where d is the solution depth.

The main difference between IDA‚àó and standard iterative deepening is that the cutoff
used is the f -cost (g + h) rather than the depth; at each iteration, the cutoff value is the small-
est f -cost of any node that exceeded the cutoff on the previous iteration.

As the recursion unwinds, RBFS replaces the f -value of each node along the path
with a backed-up value‚Äîthe best f -value of its children. In this way, RBFS remembers the
f -value of the best leaf in the forgotten subtree and can therefore decide whether it‚Äôs worth reexpanding


A‚àó tree search, RBFS is an optimal algorithm if the heuristic function h(n) is
admissible.

ffective branching factor b‚àó . If the
total number of nodes generated by A‚àó for a particular problem is N and the solution depth is
d, then b‚àó is the branching factor that a uniform tree of depth d would have to have in order
to contain N + 1 nodes. Thus,
N + 1 = 1 + b‚àó + (b‚àó )2 + ¬∑ ¬∑ ¬∑ + (b‚àó )d .

A problem with fewer restrictions on the actions is called a relaxed problem.

because the relaxed problem adds edges to the state space, any optimal solution in the
original problem is, by deÔ¨Ånition, also a solution in the relaxed problem; but the relaxed
problem may have better solutions if the added edges provide short cuts. the cost of
an optimal solution to a relaxed problem is an admissible heuristic for the original problem.
Furthermore, because the derived heuristic is an exact cost for the relaxed problem, it must
obey the triangle inequality and is therefore consistent (see page 95). 

RANDOM FROM AMONG THE UPHILL MOVES THE PROBABILITY OF SELECTION CAN VARY WITH THE STEEPNESS
OF THE UPHILL MOVE 4HIS USUALLY CONVERGES MORE SLOWLY THAN STEEPEST ASCENT

first choice hill climbing IMPLEMENTS STOCHASTIC
HILL CLIMBING BY GENERATING SUCCESSORS RANDOMLY UNTIL ONE IS GENERATED THAT IS BETTER THAN THE
CURRENT STATE 4HIS IS A GOOD STRATEGY WHEN A STATE HAS MANY EG THOUSANDS OF SUCCESSORS

Store k states instead of 1
‚Äì Hill climbing just stores the current state
‚Äì Beam (window) stores k
‚ñ™ Algorithm
‚Äì Begins with k random starts
‚Äì Each iteration generate successors for all k states
‚Äì Repeat with best k among successors unless goal found
‚ñ™ Better than k parallel random restarts
‚Äì Since best k among ALL successors taken (not best from each set of successors, k times)
‚ñ™ Stochastic beam search
‚Äì Original variant may still get stuck in a local cluster
‚Äì Adopt stochastic strategy similar to stochastic hill climbing to increase state diversity

The outcome of MAX can only be the same or better if MIN plays suboptimally
compared to MIN playing optimally. So, in general, it seems like a good idea
to use minimax. However, suppose MAX assumes MIN plays optimally and
minimax determines that MIN will win. In such cases, all moves are losing and
are ‚Äúequally good,‚Äù including those that lose immediately. A better algorithm
would make moves for which it is more difficult for MIN to find the winning
line